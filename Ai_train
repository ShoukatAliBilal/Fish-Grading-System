import os
import json
import numpy as np
import cv2
from PIL import Image
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as T
from torchvision.models.detection import maskrcnn_resnet50_fpn
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor

class FishDataset(Dataset):
    def __init__(self, root, annotation_folder, transforms=None, limit=None):
        self.root = root
        self.transforms = transforms

        # Load the JSON annotations from multiple files
        self.annotation_data = {}
        annotation_files = [f for f in os.listdir(annotat

                                                  ion_folder) if f.endswith('.json')]
        for file in annotation_files:
            with open(os.path.join(annotation_folder, file)) as f:
                data = json.load(f)
                self.annotation_data.update(data)

        # Get image names from the combined annotation keys
        self.imgs = list(self.annotation_data.keys())

        if limit:
            self.imgs = self.imgs[:limit]  # Limit to the first 'limit' images

    def __getitem__(self, idx):
        img_name = self.imgs[idx]
        img_path = os.path.join(self.root, img_name)
        img = Image.open(img_path).convert("RGB")

        # Get the annotation for this image
        regions = self.annotation_data[img_name]["regions"]

        boxes = []
        labels = []
        masks = []

        for region_id, region in regions.items():
            shape_attrs = region['shape_attributes']
            all_points_x = shape_attrs['all_points_x']
            all_points_y = shape_attrs['all_points_y']

            # Create polygon mask
            poly = [(x, y) for x, y in zip(all_points_x, all_points_y)]
            poly = np.array(poly, dtype=np.int32)

            # Create mask
            mask = np.zeros((img.height, img.width), dtype=np.uint8)
            cv2.fillPoly(mask, [poly], 1)
            masks.append(mask)

            # Create bounding box
            xmin = min(all_points_x)
            xmax = max(all_points_x)
            ymin = min(all_points_y)
            ymax = max(all_points_y)
            # Create masks for multiple objects
            #masks.append(mask)
            boxes.append([xmin, ymin, xmax, ymax])
            labels.append(1)  # Assuming fish is the only class

            #boxes.append([xmin, ymin, xmax, ymax])


            #labels.append(1)  # Assuming "fish" label is 1

        # If no boxes are found, skip this image
        if len(boxes) == 0:
            print(f"No annotations found for image {img_name}, skipping.")
            next_idx = (idx + 1) % len(self.imgs)
            return self.__getitem__(next_idx)

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        image_id = torch.tensor([idx])

        target = {"boxes": boxes, "labels": labels, "masks": masks, "image_id": image_id}

        if self.transforms:
            img = self.transforms(img)

        return img, target

    def __len__(self):
        return len(self.imgs)

# Define transformations
def get_transform(train):
    transform_list = []
    transform_list.append(T.ToTensor())
    if train:
        transform_list.append(T.RandomHorizontalFlip(0.5))
    return T.Compose(transform_list)

# Initialize the dataset
train_dataset = FishDataset(
    root='/content/drive/MyDrive/programs/train_B',
    annotation_folder='/content/drive/MyDrive/programs/annotations_tr',  # Folder instead of file
    transforms=get_transform(train=True)
)

val_dataset = FishDataset(
    root='/content/drive/MyDrive/programs/val_B',
    annotation_folder='/content/drive/MyDrive/programs/annotations_val',  # Folder instead of file
    transforms=get_transform(train=False)
)

# DataLoader
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))
val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))

# Get the model using Mask R-CNN
def get_instance_segmentation_model(num_classes):
    model = maskrcnn_resnet50_fpn(pretrained=True)
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels
    hidden_layer = 256
    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)

    return model

model = get_instance_segmentation_model(num_classes=2)
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# Training parameters
params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

# Training loop
num_epochs = 100
i = 0
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for images, targets in train_loader:
        images = list(image.to(device) for image in images)
        # Ensure targets are properly structured dictionaries
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # Check if target is a dictionary
        if isinstance(targets, list):
            for target in targets:
                if not isinstance(target, dict):
                    raise TypeError("Each target must be a dictionary")

        loss_dict = model(images, targets)
        losses = sum(loss for loss in loss_dict.values())

        optimizer.zero_grad()
        losses.backward()
        i += 1
        print("done", i)
        optimizer.step()

        running_loss += losses.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}")

# Evaluate on the validation set
model.eval()
with torch.no_grad():
    val_loss = 0.0
    for images, targets in val_loader:
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # Check if the model is in training mode or evaluation mode
        model.train()  # Ensure the model is in training mode to calculate loss
        loss_dict = model(images, targets)
        print(loss_dict)

        # Now we check whether loss_dict is a list (inference mode) or dict (training mode)
        if isinstance(loss_dict, dict):
            losses = sum(loss for loss in loss_dict.values())
            val_loss += losses.item()
        else:
            # If we get a list, that indicates the model is giving predictions, not losses.
            print("Expected a loss dictionary, but got predictions instead.")
            # You might want to break the loop here or handle it as needed
            break

    print(f"Validation Loss: {val_loss/len(val_loader):.4f}")


# Save the model
# Define the directory where you want to save the model
save_directory = '/content/drive/MyDrive/programs/model'  # Replace with your desired directory

# Ensure the directory exists; if not, create it
os.makedirs(save_directory, exist_ok=True)

# Define the full path including the file name
model_save_path = os.path.join(save_directory, 'fish_detection_model7.pth')

# Save the model state dictionary to the specified path
torch.save(model.state_dict(), model_save_path)

print(f"Model saved to {model_save_path}")
print("Model training complete.")
